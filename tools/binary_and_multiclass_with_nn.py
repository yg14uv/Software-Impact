# -*- coding: utf-8 -*-
"""Binary and Multiclass with NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i5F8mwBMR68_1L1WMkoZkepAQ90nE52J

# Import Libraries
"""

#import useful Libraries
import tensorflow
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder,MinMaxScaler,normalize
from sklearn.metrics import confusion_matrix,classification_report,recall_score,f1_score,precision_score,precision_recall_curve,roc_curve
from keras.models import Sequential,Model
from keras.layers import Dense,BatchNormalization,Activation,Input,LeakyReLU
#from keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam
from keras.callbacks import ReduceLROnPlateau
#from keras.utils import to_categorical
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
# warnings.filterwarnings("ignore")

"""# Part A: Binary Classification

## Dataset Wrangling
---

Extracting the labels from the dataset and removing the columns which have a datatype of 'object'. This is because the type object is mostly strings and does not contribute to the training of the model.
"""

df = pd.read_csv('IoTID20.csv')
labels = df.iloc[:,-3]
df = df.select_dtypes(exclude=['object'])

df.info()

"""We have two classes in the dataset. Normal and Anomly. Since it still have the datatype as object and would produce incorrect results while training the model. We have to encode the label classes as well to convert them into integers. Since we have only two classes, the labels would be 0 and 1 for anomly and normal respectively."""

le = LabelEncoder()
en_labels = le.fit_transform(labels)
le.classes_

#Concatinating the labels with the dataset
df['Labels'] = en_labels
df.head()

"""Here, we checked whether the dataset contains any infinite or NaN values, if yes, they have to be removed in order to produce efficient results."""

print("Does dataframe have inf values: {}".format(np.any(np.isinf(df))))
print("Does dataframe have nan values: {}".format(np.any(np.isnan(df))))
print("Dataframe shape before dropping na/inf: {}".format(df.shape))
df.replace(to_replace=[np.inf,-np.inf],value=np.nan,inplace=True)
df.dropna(inplace=True)
print("Dataframe shape after dropping na/inf: {}".format(df.shape))
print("Does dataframe still have inf values: {}".format(np.any(np.isinf(df))))
print("Does dataframe still have nan values: {}".format(np.any(np.isnan(df))))

"""Seperating the labels from the dataset as X and Y datasets."""

X = df.iloc[:,:-1]
X.head()

Y = df.iloc[:,-1]
Y.head()

"""This dataset is huge and contains large values which eventually takes a lot of time to train the model and even after that the accuracy is not that create. This is why we had to normalize the dataset so the each feature of the input would contain smaller values and produce efficient results."""

X = normalize(X,axis=1)
X

"""After finalizing the preprocessing, it time to split the dataset into train and test set. For this purpose, I import the train_test_split method from the `sklearn` library.

## Train and Test set split
"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y,random_state=1,test_size=0.3)
# Y_train = Y_train.values.reshape(-1,1)
# Y_test = Y_test.values.reshape(-1,1)

print("Train set X size: ",X_train.shape)
print("Train set Y size: ",Y_train.shape)
print("Test set X size: ",X_test.shape)
print("Test set Y size: ",Y_test.shape)

"""## Neural Network based Binary Classifier using Keras:
---

The model I built consisted of 3 hidden layers with 60, 30, and 15 neurons respectively. Each of the hidden layer had `relu` activation function. Where as the output layer had only 1 neuron and `sigmoid` activation function because of Binary Classification.
"""

model = Sequential()
model.add(Dense(units=60,input_dim=79,activation='relu',kernel_initializer='glorot_uniform'))
model.add(Dense(units=30,activation='relu',kernel_initializer='glorot_uniform'))
model.add(Dense(units=15,activation='relu',kernel_initializer='glorot_uniform'))
model.add(Dense(1,activation='sigmoid',kernel_initializer='glorot_uniform'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])

history = model.fit(X_train,Y_train,batch_size=100,epochs=15)

"""#### Loss and Accuracy"""

loss,accuracy = model.evaluate(X_test,Y_test)
print("Loss: {}, Accuracy: {}".format(loss,accuracy))

"""#### Predictions"""

predictions = model.predict(X_test)
predictions = predictions>0.5
predictions

"""#### Loss and Accuracy Graph"""

plt.plot(history.history['acc'])
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.show()

plt.plot(history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

"""#### F1 Score"""

print("F1: %f"% f1_score(Y_test, predictions))

"""#### Precision Score"""

print("Precision: %f "%precision_score(Y_test,predictions))

"""#### Recall Score"""

print("Recall: %f "%recall_score(Y_test,predictions))

def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], 'b--', label='precision')
    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')
    plt.xlabel('Threshold')
    plt.legend(loc='upper left')
    plt.ylim([0,1])

prec, rec, tre = precision_recall_curve(Y_test, predictions)

plot_prec_recall_vs_tresh(prec, rec, tre)
plt.show()

"""#### Confusion Matrix"""

cm = confusion_matrix(Y_test,predictions)
cm

"""#### Heat Map"""

sns.heatmap(cm,annot=True,fmt='g')

"""#### ROC CURVE"""

ns_fpr, ns_tpr, _ = roc_curve(Y_test, predictions)

plt.plot(ns_fpr, ns_tpr, linestyle='--')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the plot
plt.show()

cm = cm.ravel()
percentage_accuracy = (cm[0] + cm[-1]) / predictions.shape[0] *100
print("The accuracy we acheived from Keras Model is: {} %".format(percentage_accuracy))

"""#### Result

As you can see, Keras' simple neural network had an accuracy of 98.40%

Now let's train SVM with the same dataset for binary classification of Labels and compare it's performances with NN on the basis of accuracy.

# Part B: Multi-Class Classification

## Data Wrangling for Multi-Class Classification

For Multiclassification we are going to use Sub-Category column as suggested in the assignment documentation.
"""

df = pd.read_csv('IoTID20.csv')
labels = df.iloc[:,-1]
df = df.select_dtypes(exclude=['object'])

df.info()

"""Sinces the labels are listed as strings by default, we have to encode them first in order to use them in our classfiers. You can see that there are total 9 label classes."""

le = LabelEncoder()
en_labels = le.fit_transform(labels)
print(le.classes_)
print("Total Classes: ",len(le.classes_))

#Concatinating the labels with the dataset
df['Labels'] = en_labels
df.head()

print("Does dataframe have inf values: {}".format(np.any(np.isinf(df))))
print("Does dataframe have nan values: {}".format(np.any(np.isnan(df))))
print("Dataframe shape before dropping na/inf: {}".format(df.shape))
df.replace(to_replace=[np.inf,-np.inf],value=np.nan,inplace=True)
df.dropna(inplace=True)
print("Dataframe shape after dropping na/inf: {}".format(df.shape))
print("Does dataframe still have inf values: {}".format(np.any(np.isinf(df))))
print("Does dataframe still have nan values: {}".format(np.any(np.isnan(df))))

"""Since the SVM Classifier takes too long to train on the dataset of shape (625000,11).
We had to divide it in half before spliting it into train and test set. 
This trimming of data reduced the training time to 5 hours.
"""

size = int(df.shape[0]/2)
df = df.iloc[0:size,:]
df.shape

X = df.iloc[:,:-1]
X.head()

Y = df.iloc[:,-1]
Y.head()

"""Using MinMax Scalar to scale the feature range between 0 and 1."""

scalar = MinMaxScaler(feature_range=(0,1))
scalar.fit(X)

X = scalar.transform(X)
X

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y,random_state=1,test_size=0.3)

print("Train set X size: ",X_train.shape)
print("Train set Y size: ",Y_train.shape)
print("Test set X size: ",X_test.shape)
print("Test set Y size: ",Y_test.shape)

"""## Training NN using Keras for Multi Classification."""

model = Sequential()
model.add(Dense(units=512,input_shape=(X_train.shape[1],),activation='relu',kernel_initializer='glorot_uniform'))
model.add(Dense(units=100,activation='relu',kernel_initializer='glorot_uniform'))
model.add(Dense(units=30,activation='relu',kernel_initializer='glorot_uniform'))
model.add(Dense(units=15,activation='relu',kernel_initializer='glorot_uniform'))
model.add(Dense(9,activation='softmax',kernel_initializer='glorot_uniform'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

"""Using `to_categorical` to convert labels into One hot encoding vectors."""

Y_train = to_categorical(Y_train,9)
Y_test = to_categorical(Y_test,9)

history = model.fit(X_train,Y_train,batch_size=256,epochs=200)

"""#### Loss and Accuracy"""

loss,accuracy = model.evaluate(X_test,Y_test)
print("Loss: {}, Accuracy: {}".format(loss,accuracy))

"""#### Predictions"""

predictions = model.predict(X_test)
predictions = np.argmax(predictions, axis=1)
predictions

"""#### Loss and Accuracy Graph"""

plt.plot(history.history['accuracy'])
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.show()

plt.plot(history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

"""#### F1 Score"""

print("F1: %f"% f1_score(np.argmax(Y_test,axis=1), predictions,average="micro"))

"""#### Precision Score"""

print("Precision: %f "%precision_score(np.argmax(Y_test,axis=1),predictions,average="weighted"))

"""#### Recall Score"""

print("Recall: %f "%recall_score(np.argmax(Y_test,axis=1), predictions,average="macro"))

"""#### Confusion Matrix"""

cmNN = confusion_matrix(np.argmax(Y_test,axis=1),predictions)
print(cmNN)

"""#### Heat Map"""

sns.heatmap(cmNN,annot=True,fmt='g')

"""#### ROC CURVE

ROC Curve not supported for multi class classification

#### Result
"""

print("The accuracy of NN for Multi classification is: {}".format((cmNN.trace()/cmNN.sum()*100)))
